# coding=utf-8
import copy
import math
import logging
import codecs
import numpy as np
import sys

logging.basicConfig(level=2)
logger = logging.getLogger(__name__)


infinity = float('inf')
zero = float(1e-999)


def viterbi_score(confusion_networks):
    """
    1-best score obtained from the CN
    :param confusion_network:
    :return: best path, value for this path
    """
    for confusion_network in confusion_networks:
        prev, score = [-infinity] * len(confusion_network), [-infinity] + [0.0] * len(confusion_network)
        for t in range(0, len(confusion_network)):  # t: words in the sentence ("bfs")
            prev, score = score, prev
            for j in range(0, len(confusion_network[t])):  # Iterates deep-first in a CN position ("dfs")
                score[j] = max([prev[i] +
                                confusion_network[i][j][2]
                                for i in range(0, len(confusion_network[t]))])
    return max([score[i] for i in range(1, len(confusion_network[t]))])


def viterbi_path(confusion_networks):
    scores = []
    paths = []
    for confusion_network in confusion_networks:
        prev, score = [0.0] * len(confusion_network), [0.0] * len(confusion_network)
        backpointer = [[None] * (len(confusion_network[0]))
                       for i in range(len(confusion_network) + 1)]

        for t in range(0, len(confusion_network)):  # t: words in the sentence ("bfs")
            prev, score = score, prev
            for j in range(0, len(confusion_network[t])):  # Iterates deep-first in a CN position ("dfs")
                score[j], backpointer[t][j] = max([(prev[t] +
                                                    confusion_network[t][i][2], i)
                                                   for i in range(0, len(confusion_network[t]))])
        score[len(confusion_network) - 1], backpointer[len(confusion_network)][len(confusion_network[-1]) - 1] = \
            max([(score[i], i) for i in range(1, len(confusion_network[-1]))])

        path = [len(confusion_network[0]) - 1]  # End node

        for t in range(len(confusion_network), -1, -1):
            path.append(backpointer[t][path[-1]])
        path.reverse()
        paths.append(path)
        score = 0.0
        for node in range(0, len(path[1:-1])):
            score += confusion_network[node][path[node]][2]
        scores.append(score)
    return (scores, paths)


def viterbi_greedy_path(confusion_network):
    scores = []
    paths = []
    sentences = ''
    n_systems = len(confusion_network)
    len_row = len(confusion_network[0][0])
    sentence = ''
    global_prob = 0.0
    for word_index in range(0, len_row):
        max_prob = 0
        word = ''
        for system in range(0, n_systems):
            print 'len row:', len_row, 'real:', len(confusion_network[system][2]), ':', confusion_network[system][2]
            em = confusion_network[system][0]
            en = confusion_network[system][1]
            prob = confusion_network[system][2]
            if prob[word_index] > max_prob:
                max_prob = prob[word_index]
                word = en[word_index]  # (Esto no lo tengo nada claro xD)
        sentence += str(word) + ' '
        global_prob += max_prob
    print sentence

    return (scores, paths, sentences)


def voteConfusionNetwork(confusion_networks):
    """
    We sum up the probabilities of the arcs which are labeled with the same word
    and have the same start and the same end state.
    These probabilities are the global probabilities assigned to the different MT systems.
    They are manually adjusted based on the performance of the involved MT systems on a held-out development set.
    In general, a better consensus translation can be produced if the words hypothesized by a better-performing
    system get a higher probability. Additional scores like word confidence measures can
    be used to score the arcs in the lattice.

    :param confusion_networks:
    :return:
    """

    return


def replace_at_index(tup, ix, val):
    """
    Replaces the value at an index of a tuple
    :param tup: Tuple
    :param ix: Index
    :param val: Value
    :return: The modified tuple
    """

    return tup[:ix] + (val,) + tup[ix + 1:]


def read_sentences(snt_file):
    """
    Reads sentences from a snt file generated by Giza
    :param snt_file: The .snt file
    :return: Sequence of word indices, one for the en side an other one for the fr side
    """

    sentences = open(snt_file).readlines()
    em = []
    en = []
    i = 0

    for line in sentences:
        line = line.split()
        if line[0] != '1':
            em_snt = []
            en_snt = []
            if i % 3 == 1:
                for word in line:
                    en_snt += [int(word)]
                en.append(en_snt)
            elif i % 3 == 2:
                for word in line:
                    em_snt += [int(word)]
                em.append(em_snt)
        i += 1
    return em, en


def read_alignments(alignments_filename):
    """
    Returns a list of dictionaries:
    The list is indexed by the french word.
	Dictionaries' keys are the english words. Values are the probabilities.
	    E.g: list[21] == {32:0.01}
    This means that french word 21 is aligned with english word 32 with probability 0.01

    :param alignments_filename: File of the alignments
    :return: Alignments dictionary
    """

    f_aligns = {}
    alignments = open(alignments_filename).readlines()
    n_french_words = int(alignments[len(alignments) - 1].split()[0]) + 1
    align = [{}] * n_french_words
    prev_word = 1
    for line in alignments:
        line = line.split()
        french = int(line[0])
        english = int(line[1])
        prob = float(line[2])

        if french == prev_word:
            f_aligns[english] = prob
        else:
            align[prev_word] = f_aligns
            prev_word = french
            f_aligns = {english: prob}
    align[prev_word] = f_aligns
    return align


def index2words(index_sentence, vcb_file):
    """
    Reads a sentence and a vcb_file and returns the words in the sentence
    :@type list index_sentence:
    :@type string vcb_file:
    :return:sentence
    """

    sentence = ''
    indx_dict = {}
    vcb = open(vcb_file).readlines()
    for line in vcb:
        line = line.split()
        indx_dict[int(line[0])] = line[1]

    for word in index_sentence:

        if word == -1:
            sentence += '_eps_' + ' '
        else:
            sentence += indx_dict[word] + ' '
    return sentence


def create_index_dict(vcb_file):
    """
    Creates a dictionary from a vocabulary file from Giza
    :rtype : dict
    :@type string vcb_file: Vocabulary filename
    :return: Dictionary
    """
    index_dict = {}
    vcb = open(vcb_file).readlines()
    for line in vcb:
        line = line.split()
        index_dict[int(line[0])] = line[1]
    return index_dict


def index2word(index_word, index_dict):
    """
    Reads an index and a vcbFile and returns the word corresponding to such index
    :param index_word: Word of the query
    :param index_dict: Dictionary index -> words
    :return: Word
    """
    if index_word == -1 or index_word not in index_dict.keys():
        return '_eps_'
    else:
        return index_dict[index_word]


def levenshtein(source, target):
    """
    Computes the levenshtein distance between two words
    :rtype : int
    :@type string source: Source word
    :@type string target: Target word
    :return: Levenshtein distance
    """
    if len(source) < len(target):
        return levenshtein(target, source)

    # So now we have len(source) >= len(target).
    if len(target) == 0:
        return len(source)

    # We call tuple() to force strings to be used as sequences
    # ('c', 'a', 't', 's') - numpy uses them as values by default.
    source = np.array(tuple(source))
    target = np.array(tuple(target))

    # We use a dynamic programming algorithm, but with the
    # added optimization that we only need the last two rows
    # of the matrix.
    previous_row = np.arange(target.size + 1)
    for s in source:
        # Insertion (target grows longer than source):
        current_row = previous_row + 1

        # Substitution or matching:
        # Target and source items are aligned, and either
        # are different (cost of 1), or are the same (cost of 0).
        current_row[1:] = np.minimum(
            current_row[1:],
            np.add(previous_row[:-1], target != s))

        # Deletion (target grows shorter than source):
        current_row[1:] = np.minimum(
            current_row[1:],
            current_row[0:-1] + 1)

        previous_row = current_row

    return previous_row[-1]


def reorder(alignment_filename, snt_file, vcb_file_em, vcb_file_en):
    """
    Computes the reorderings
        1. Find, for each word \in E_n (en) which is the most probable alignment with the words \in E_m (em)
            1.1 If it is aligned with no one, --> -1
        2. If the word em has been already aligned with en, forbid it
        3. Sort En according the order of Em
    If two words \in En are aligned with the same word in Em, we keep the order of En
    :param alignment_filename: Giza alignments file name
    :param snt_file: Sentences file in Giza format
    :param vcb_file_em: Primary hypothesis vocabulary file
    :param vcb_file_en: Secondary hypothesis vocabulary file
    :return: Reorderings
    """

    global em_w_index, em_w_index
    em_sentences, en_sentences = read_sentences((str(snt_file) + '.snt'))
    alignments = read_alignments(alignment_filename)
    reordered_file = []
    logger.info(' Reordering alignments')
    logger.debug('Direction: ' + str(snt_file))
    logger.debug('Em file: ' + str(vcb_file_em))
    logger.debug('En file: ' + str(vcb_file_en))
    index_dict_em = create_index_dict(vcb_file_em)
    index_dict_en = create_index_dict(vcb_file_en)
    # For all sentences in the corpus...
    for n_sentence in range(0, len(en_sentences)):
        logger.log(1, '===========================')
        en_sentence = en_sentences[n_sentence]
        em_sentence = em_sentences[n_sentence]
        alignment = [-1] * (len(en_sentence))  # List where we store a tuple (pos_word_in_Em)
        index = 0
        # 1. Find, for each word \in E_n (en) which is the most probable alignment with the words \in E_m (em)
        already_aligned = [[]] * len(em_sentence)
        for en_index in en_sentence:
            logger.log(1, str(en_index) + ' / ' + str(len(alignments)))
            aligns = copy.copy(alignments[en_index])  # aligns: Dictionary of alignments for current En word
            max_prob = -1
            min_index = -1
            n_aligns = 0
            current_alignment_index = 0
            while n_aligns < len(en_sentence):
                for em_w_index in range(0, len(em_sentence)):
                    can_align = True
                    em = em_sentence[em_w_index]
                    if em in aligns.keys():
                        # 2. If the word em has been already aligned with en, forbid it
                        logger.log(1, str(en_index) + ' is in ' + str(already_aligned) + ' ?')
                        if en_index in already_aligned[em_w_index]:
                            # do not align and prevent it:
                            can_align = False
                            logger.log(1, 'CAN\'T ALIGN because of ' + str(en_index) + ' at position ' +
                                       str(em_w_index) + ' (word ' + str(em) + ')')
                        else:
                            if aligns[em] > max_prob:
                                if em_w_index >= current_alignment_index:
                                    # Monotone alignments
                                    max_prob = aligns[em]
                                    min_index = em_w_index
                                    current_alignment_index = min_index
                                    can_align = True
                                else:
                                    can_align = False

                if em_w_index == len(em_sentence) - 1:
                    if not can_align:  # If we are not able to align with anyone
                        alignment[index] = (-1, en_index, 1.)
                        n_aligns += 1
                        can_align = True
                        break
                if max_prob == -1:
                    if index > 0:  # Two words in En which are aligned to the same word
                        # in Em are kept in the original order.
                        assert isinstance(en_index, int)
                        alignment[index] = (-alignment[index - 1][0], en_index, 0.)
                    else:
                        alignment[index] = (0, en_index, 0.)
                    n_aligns += 1
                    break
                elif can_align:  # Assign the alignment
                    alignment[index] = (min_index, en_index, max_prob)
                    # And update the aligned words
                    already_aligned[min_index] = already_aligned[min_index] + [en_index]
                    n_aligns += 1
                    logger.log(1, ' already aligned: ' + str(already_aligned))
                    break

            index += 1

        # 3. Sort En according the order of Em
        # If two words \in En are aligned with the same word in Em, we keep the order of En
        sorted_align = sorted(alignment, key=lambda tup: math.fabs(tup[0]))
        sorted_align_words = copy.copy(sorted_align)

        for index in range(0, len(sorted_align_words)):
            em_word = em_sentence[sorted_align[index][0]]
            sorted_align_words[index] = replace_at_index(sorted_align_words[index], 0,
                                                         index2word(em_word, index_dict_en))
            sorted_align_words[index] = replace_at_index(sorted_align_words[index], 1,
                                                         index2word(sorted_align[index][1], index_dict_em))
        for index in range(0, len(sorted_align_words) - 1):
            if sorted_align_words[index][0] == sorted_align_words[index + 1][0]:
                # In case of many-to-one connections in a of words in Em
                # in to a single word from En, we only keep the connection with the lowest alignment costs.
                if sorted_align_words[index][2] > sorted_align_words[index + 1][2]:
                    sorted_align_words[index + 1] = replace_at_index(sorted_align_words[index + 1], 0, '_eps_')
                else:
                    sorted_align_words[index] = replace_at_index(sorted_align_words[index], 0, '_eps_')

        logger.log(1, ' Sentence ' + str(n_sentence + 1) + ': ' + str(em_sentence))
        logger.log(1, ' Sentence ' + str(n_sentence + 1) + ': ' + str(sorted_align))
        logger.log(1, ' Sorted ' + str(n_sentence + 1) + ': ' + unicode.encode(unicode(sorted_align_words), 'utf-8'))
        reordered_file.append(sorted_align_words)
    logger.info(' Alignments reordered')

    return reordered_file


def build_confusion_networks(reorderings_full, n_system):
    """
    A Confusion Network (CN) G is a weighted directed graph
    with a start node, an end node, and word labels over its edges.
    The CN has the peculiarity that each path from the start node
    to the end node goes through all the other nodes. As shown in
    Figure 1, a CN can be represented as a matrix of words whose
    columns have different depths.

    :rtype : List of lists
    Our confusion network will be a list of lists of lists, where:
    CF[list]: list is a list of confusion networks, one for each test sentence
    CF[list][list2]; List 2 is the confusion network of each test sentence. It is composed
    by M lists (rows in the confusion matrix)
    CF[list][list2][list3]; List 3 is a particular column of a CN (confusion_column at the code)


    Starting from an initial state s_0, the primary hypothesis is
    processed from left to right and a new state is produced for each word e_{n,i}.
    Then, an arc is created from the previous state to this state, for e_{n,i} and for all words
    (or the null word) aligned to e_{n,i}. If there are insertions following e_{n,i} (for example,
    "have some" in Fig. 3), the states and arcs for the inserted words are also created.

    :param reorderings_full: Reorderings structure
    :param n_system: Number of the system
    :return: confusion_network
    """

    n_systems = len(reorderings_full)
    n_sentences = len(reorderings_full[0])
    logger.info(' Building confusion networks. Counting ' + str(n_systems + 1) + ' systems')
    # For each sentence in the test set
    #for sentence_index in range(0, n_sentences):
    for sentence_index in [43]:#range(1113, 1115):
        for system_index in range(0, n_systems):

            reorderings = copy.copy(reorderings_full[system_index])
            reordering = reorderings[sentence_index]
            try:
                f = codecs.open(out_dir + '/sentence_' + str(sentence_index + 1) + '_system_' + str(n_system) + '.CN',
                                mode='r', encoding='utf-8')
                cn = f.readlines()
                f.close()
            except:
                cn = [''] * len(reordering)
            out = codecs.open(out_dir + '/sentence_' + str(sentence_index + 1) + '_system_' + str(n_system) + '.CN',
                              mode='w', encoding='utf-8')

            state = -1
            len_reordering = len(reordering)
            while state < len_reordering - 1:
                state += 1
                if len(cn) > state:
                    line = cn[state][:-1]
                else:
                    line = ''


                """
                # However, a better correspondence can be achieved when we ensure that
                # identical words are aligned with each other. To this end, we compute
                # the edit distance alignment between all the insertions of the
                # secondary translations.
                if reordering[state][0] == '_eps_':  # If it is an insertion...
                    for state3 in  range(0, len(reordering)):
                        min_dist = infinity
                        min_word = reordering[state3][0]
                        min_state = -1
                        min_prob = 0.0

                        for system_index2 in range(0, n_systems):  # We look in all secondary systems
                            # Extract the reordered alignments of the system

                            if not system_index == system_index2:
                                reorderings2 = reorderings_full[system_index2]
                                reordering2 = reorderings2[sentence_index]

                                for state2 in range(0, len(reordering2)):
                                    #if reordering[state2][0] == '_eps_':  # We look in all insertions
                                        # We compute the edit distance between all insertions
                                    edit_dist = levenshtein(reordering[state2][1], min_word)
                                    # logger.debug('Testing: ' + str(min_word) + ' against: ' + reordering[state2][1] + '. Edit distance: ' + str(edit_dist))
                                    if edit_dist < min_dist:
                                        min_dist = edit_dist  # Minimum edit distance
                                        min_word = reordering[state2][1]  # Word of the secondary hypothesis
                                        # which provide the minimum edit distance
                                        min_prob = reordering[state2][2]
                                        min_state = state2  # Index of the pair (hypothesis, word)
                            #logger.debug('Insertion modified with ' + str(min_word))
                            # logger.debug(' ------------------------------------- ')
                            # Up to here, we've got the word that should be aligned
                            # We replace it now in the reordered alignments
                                #logger.debug('Before: ' + str(reordering[state3]))
                                reordering[state] = replace_at_index(reordering[state3], 1, min_word)
                                #logger.debug('After: ' + str(reordering[state3]))#reordering[state][0] + ' aligned with ' + reordering[min_state][1])
                                reordering[min_state] = (
                                    reordering[min_state][0], min_word,
                                    min_prob)  # replace_at_index(reordering[state],0, min_word)
                                len_reordering = len(reordering)




                """




                if reordering[state][0] != '':
                    if reordering[state][0] == reordering[state][1]:
                        line = line.encode('utf-8') + ' ' + reordering[state][0] + ' ' + str(
                            reordering[state][2]) + '\n'
                    else:
                        line = line.encode('utf-8') + ' ' + reordering[state][0] + ' ' + str(
                            reordering[state][2]) + ' ' + reordering[state][1] + ' ' + str(reordering[state][2]) + '\n'
                    out.write(line.decode('utf-8'))

            out.close()





            lines = codecs.open(out_dir + '/sentence_' + str(sentence_index + 1) + '_system_' + str(n_system) + '.CN',
                                mode='r', encoding='utf-8').readlines()
            out = codecs.open(out_dir + '/sentence_' + str(sentence_index + 1) + '_system_' + str(n_system) + '.CN',
                              mode='w', encoding='utf-8')
            # out_joint = codecs.open(out_dir + '/sentence_' + str(sentence_index + 1) + '_all' + '.CN',
            # mode='w', encoding='utf-8')
            #line_joint = ''
            #print "-----------------------------------------Reorganizando la linea:", lines
            for line in lines:
                #print line
                #print '=============================================================='
                line_split = line.split()
                line_probs = {}
                line_counts = {}

                for i in range(0, len(line_split) - 1, 2):
                    if line_split[i] in line_probs.keys():
                        line_probs[line_split[i]] += float(line_split[i + 1])
                        line_counts[line_split[i]] += 1
                    else:
                        line_probs[line_split[i]] = float(line_split[i+1]) #RRRevisar esto!
                        line_counts[line_split[i]] = 1
                new_line = ''

                # maxim = sum(float(line_probs[key]) for key in line_probs.keys())

                # if maxim == 0.0:

                #     maxim = 1.0
                for word in line_probs.keys():
                    new_line += word + ' ' + str(line_probs[word] ) + ' '  # / line_counts[word]) + ' '
                #print new_line
                out.write(new_line + '\n')
                #line_joint = line_joint + ' '  + new_line
                #out_joint.write(line_joint + '\n')
            out.close()
            #out_joint.close()

    return n_sentences


def decode_networks(n_systems, n_sentences, output_file_prefix):
    """
    Decode confusion networks

    :rtype : None
    :param n_systems: Number of systems
    :param n_sentences: Number of sentences
    :param out_dir: Output directory
    :param output_file_prefix: Prefix of the output files
    :return: Decoded test file
    """

    logger.info('Decoding confusion networks')
    outputFile2 = codecs.open(output_file_prefix + '/' + 'combined' + '.hyp', mode='a', encoding='utf-8')
    for sentence_index in range(0, n_sentences):
        logger.log(1, 'Sentence: ' + str(sentence_index))
        # For each sentence in the test set
        max_sent_prob = -infinity
        max_sent = ''

        for system_index in range(0, n_systems):
            confusion_network = codecs.open(
                out_dir + '/sentence_' + str(sentence_index + 1) + '_system_' + str(system_index + 1) + '.CN', mode='r',
                encoding='utf-8')
            outputFile = codecs.open(
                output_file_prefix + '/' + str(task_names[system_index][0][1]).split('/')[-1] + '.hyp', mode='a',
                encoding='utf-8')
            logger.log(1, 'Storing results in file ' + output_file_prefix + '/' +
                       str(task_names[system_index][0][1]).split('/')[-1] + '.hyp')
            sentence = ''
            sentence_prob = 1.0
            n_words = 0
            prev_word = ''
            for line in confusion_network.readlines():
                line = line.split()
                max_word = ''
                max_prob = -infinity
                for i in range(0, len(line), 2):
                    word = line[i]
                    prob = float(line[i + 1])
                    # Otra heurística "mala"...: elegir transicion vacía si es posible
                    if prob == max_prob and word == '_eps_':
                        max_word = word

                    if prob > max_prob:
                        max_word = word
                        max_prob = prob

                # Heuristica: No repetir palabras
                # lo hacen también en el artículo de Matusov y Schwenk
                if prev_word == max_word:
                    max_word = ''
                else:
                    prev_word = max_word

                if max_word != '' and max_word != '_eps_':
                    sentence += max_word + ' '
                    n_words += 1
                sentence_prob += max_prob
            outputFile.write(sentence + '\n')
            sentence_prob /= n_words
            if sentence_prob > max_sent_prob:
                max_sent = sentence
                max_sent_prob = sentence_prob
        #logger.log(5, 'Max prob sentence ' + str(sentence_index + 1) + ': ' + str(max_sent_prob))
        outputFile2.write(max_sent + '\n')

    outputFile.close()
    outputFile2.close()
    return


if __name__ == '__main__':

    prefix = sys.argv[1]

    out_dir = prefix + '/sentences'
    # task_names format: [(em_en, em, en)]

    task_names = [
        [(prefix + '/alignments/nmt_thot', prefix + '/alignments/nmt', prefix + '/alignments/thot'),
         (prefix + '/alignments/nmt_moses', prefix + '/alignments/nmt', prefix + '/alignments/moses')],

        [(prefix + '/alignments/moses_thot', prefix + '/alignments/moses', prefix + '/alignments/thot'),
         (prefix + '/alignments/moses_nmt', prefix + '/alignments/moses', prefix + '/alignments/nmt')],

        [(prefix + '/alignments/thot_nmt', prefix + '/alignments/thot', prefix + '/alignments/nmt'),
         (prefix + '/alignments/thot_moses', prefix + '/alignments/thot', prefix + '/alignments/moses')]
    ]


    sntFile = []
    confusion_network = [[]] * len(task_names)

    a = 0
    i = 0
    M = len(task_names)
    for em in range(0, M):
        reorders = []
        for en in range(0, len(task_names[em])):
            # sntFile.append(str(task_names[em][en][0]))
            sntFile = str(task_names[em][en][0])
            coocFile = str(task_names[em][en][0]) + '.cooc'
            alignments = task_names[em][en][0] + '.t3.final'
            vcbFile_em = task_names[em][en][1] + '.vcb'
            vcbFile_en = task_names[em][en][2] + '.vcb'
            # print task_names[em][en][0], '---> ', vcbFile_em, vcbFile_en
            reorders.append(reorder(alignments, sntFile, vcbFile_em, vcbFile_en))
        a += 1
        n_sentences = build_confusion_networks(reorders, a)

    decode_networks(M, n_sentences,  prefix)